import uuid
from openai import OpenAI
from tqdm import tqdm
from pinecone import Pinecone
from configparser import ConfigParser

config = ConfigParser()
config.read('config.ini')

pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index("data")

client = OpenAI(api_key=OPENAI_API_KEY)

messages = []
retrieved_chunks = []
summarized_query = ""
system_prompt = {
    "role": "system",
    "content": """ë‹¹ì‹ ì€ ìœ ì €ê°€ í´ë¦¬í•‘í•œ ì‚¬ì´íŠ¸ì—ì„œ ë‚˜íƒ€ë‚œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë‹µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì€ ì‚¬ëŒë“¤ì—ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:
    - ì¹œì ˆí•œ ë§íˆ¬
    - í•­ìƒ ì¡´ëŒ“ë§ ì‚¬ìš©
    - ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©
    ë‹¹ì‹ ì€ ë°˜ë“œì‹œ ì œê³µí•˜ëŠ” [Context]ì— ìˆëŠ” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚´ì„ ë¶™ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤."""
}
messages.append(system_prompt)

def handle_user_input(prompt):
    user_prompt = {
        "role": "user",
        "content": prompt
    }
    messages.append(user_prompt)
    print("User:", prompt)

    recent_query = ""
    for message in messages[-3:]:
        recent_query += f'{message["role"]}"\n"{message["content"]}"\n\n"'

    recent_query_prompt = {
        "role": "user",
        "content": f"2 pair of question-answers.\n{recent_query}"
    }

    summarization_system_prompt = {
        "role": "system",
        "content": "ìœ ì €ê°€ ì…ë ¥í•œ ì—¬ëŸ¬ ì§ˆë¬¸ì„ íŒŒì•…í•˜ê³  ìœ ì €ê°€ ê¶ê¸ˆí•œ ê²ƒì„ í•˜ë‚˜ì˜ ê¸´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•´ì¤˜. ëŒ€í™” ì¤‘ê°„ì— ì£¼ì œê°€ ë°”ë€Œì—ˆìœ¼ë©´ ìœ ì €ê°€ ì…ë ¥í•œ ìµœì‹  ì£¼ì œì— ë§ì¶°ì„œ ì •ë¦¬í•´ì¤˜."
    }

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        messages=[summarization_system_prompt, recent_query_prompt],
        temperature=0.5,
        max_tokens=1024,
    )

    summarized_query = response.choices[0].message.content
    print("Summarized Query:", summarized_query)

    # Query Embeddings ìƒì„±
    response = client.embeddings.create(input=prompt, model="text-embedding-3-small")
    query_embeddings = response.data[0].embedding

    # Contexts ê²€ìƒ‰
    retrieved_chunks = index.query(
        namespace="hook",
        vector=query_embeddings,
        top_k=7,
        include_values=False,
        include_metadata=True,
    )
    contexts = ""

    for i, match in enumerate(retrieved_chunks.matches, start=1):
        context = match["metadata"]["chunk"]
        print(f"ì°¸ì¡°{i}: {context}\n")
      
        contexts += context + "\n\n"

    context_prompt = {
        "role" : "system",
        "content" : f"[Context]\n{contexts}"
    }
    messages.append(context_prompt)

    # print('messages:', messages)

    # ì±—ë´‡ì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
    full_response = client.chat.completions.create(
          model="gpt-3.5-turbo-0125",
          messages=messages,
          temperature=0.5,
          max_tokens=1024,
    ).choices[0].message.content
    print("Assistant:", full_response)
    messages.append({"role": "assistant", "content": full_response})

while True:
    prompt = input("í´ë¦¬í•‘í•œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¶ê¸ˆí•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”ğŸ˜Š: ")
    handle_user_input(prompt)
